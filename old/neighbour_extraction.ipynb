{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraktion der Daten:\n",
    "- basierend auf example.py werden hier Daten extrahier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finden von Occurences\n",
    "\n",
    "Aus stop_times.txt werden für jeden Stop aus stops.txt Zeilen extrahiert, in denen der stop vorkommt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_occ_numbers():\n",
    "    print(\"finding occurencies\")\n",
    "    stop_times = load_stop_times()\n",
    "    line_numbers = {k: [] for k in [i[0] for i in load_stops()]}\n",
    "    c = time.perf_counter()\n",
    "    new_list = []\n",
    "    for ind, elem in enumerate(stop_times):\n",
    "        new_list.append((elem[3], ind))\n",
    "    #new_list = sorted(new_list, key=lambda x: x[0])\n",
    "    for elem in new_list:\n",
    "        line_numbers[elem[0]].append(elem[1])\n",
    "    with open(\"lvb_auswertung/occ_numbers.json\", \"w+\") as f:\n",
    "        json.dump(line_numbers, f, indent=4)\n",
    "    print(\"found occurencies\")\n",
    "\n",
    "def load_all_occ_numbers():\n",
    "    with open(\"lvb_auswertung/occ_numbers.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finden von Nachbarn mit minimaler Transferzeit\n",
    "\n",
    "Nun finden wir für jede Occurency und jede Station heraus, welche Nachbarn und welche minimale Transferzeit sie zu einem Nachbarn hat.\n",
    "\n",
    "Zunächst benötigen wir dazu zwei Hilfsfunktionen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_sec(time):\n",
    "    time = time.replace('\"', '')\n",
    "    return 3600*int(time[0:2])+60*int(time[3:5])+int(time[6:8])\n",
    "\n",
    "def find_neighbours(stop_times, occ_numbers):\n",
    "    neighbours = []\n",
    "    neighbour_times = []\n",
    "    for occ_number in occ_numbers:\n",
    "        point = stop_times[occ_number]\n",
    "        point_arr = time_sec(stop_times[occ_number][1])\n",
    "        point_dep = time_sec(stop_times[occ_number][2])\n",
    "        if occ_number != 0:\n",
    "            before = stop_times[occ_number-1]\n",
    "            diff = point_arr-time_sec(stop_times[occ_number-1][2])\n",
    "            if before[0] == point[0]:\n",
    "                neighbour_times.append((before[3], diff))\n",
    "                neighbours.append(before[3])\n",
    "        \n",
    "        if occ_number != len(stop_times)-1:\n",
    "            after = stop_times[occ_number+1]\n",
    "            diff = time_sec(stop_times[occ_number+1][1])-point_dep\n",
    "            if after[0] == point[0]:\n",
    "                neighbour_times.append((after[3], diff))\n",
    "                neighbours.append(after[3])\n",
    "    neighbours = list(set(neighbours))\n",
    "    def min_time(neighbour):\n",
    "        min_list = [elem[1] for elem in neighbour_times if elem[0] == neighbour]\n",
    "        min_list = [elem for elem in min_list if elem != 0]\n",
    "        if len(min_list) == 0:\n",
    "            return 0\n",
    "        return min(min_list)\n",
    "    return [(\n",
    "                neighbour, min_time(neighbour)\n",
    "            ) for neighbour in neighbours]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erklärung der Funktionen:\n",
    "\n",
    "- time_sec:\n",
    "  - Eine Zeit im Format \"hh:mm:ss\" wird in Sekunden umgerechnet\n",
    "  - Die Differenzen können einfach zwischen der arrival und departure-Zeit errechnet werden, findet eine Fahrt um Mitternacht statt, gibt es Uhrzeiten mit Stunde 24 und 25, daher kann diese Funktion für Differenzen verwendet werden.\n",
    "\n",
    "- find_neighbours: ermittelt für einen Stop alle Nachbarn mit minimaler Transferzeit\n",
    "  - Die Liste mit allen stop_times und occurency_numbers für einen Stop wird übergeben\n",
    "  - für jede occurency eines Stops wird ermittelt, welcher Stop davor und danach (before und after) angefahren wird (index davor bzw. danach falls vorhanden, gleicher Trip)\n",
    "  - zu einer Liste wird der Nachbar hinzugefügt, zu einer anderen der Nachbar und eine Zeitdifferenz\n",
    "  - zuletzt wird für jeden Nachbar die minimale Transferzeit größer als 0 extrahiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ermittlung aller Nachbarn\n",
    "\n",
    "Anschließend wird die find_neighbours-Funktion für jeden Stop aus stops.txt ausgeführt, die gefundenen Nachbarn und Zeiten werden gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_neighbours():\n",
    "    print(\"loading data...\")\n",
    "    stop_ids = [i[0] for i in load_stops()]\n",
    "    stop_times = load_stop_times()\n",
    "    occ_numbers = load_all_occ_numbers()\n",
    "    all_neighbours = {}\n",
    "    print(\"finding neighbours...\")\n",
    "    #for elem in stop_ids[:1]:\n",
    "    for elem in tqdm(stop_ids):\n",
    "        all_neighbours[elem] = find_neighbours(stop_times, occ_numbers[elem])\n",
    "    with open(\"lvb_auswertung/neighbours.json\", \"w+\") as f:\n",
    "        json.dump(all_neighbours, f, indent=4)\n",
    "    print(\"finished...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding occurencies\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_stop_times' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_occ_numbers()\n\u001b[0;32m      2\u001b[0m all_neighbours()\n",
      "Cell \u001b[1;32mIn [3], line 3\u001b[0m, in \u001b[0;36mall_occ_numbers\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_occ_numbers\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinding occurencies\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     stop_times \u001b[39m=\u001b[39m load_stop_times()\n\u001b[0;32m      4\u001b[0m     line_numbers \u001b[39m=\u001b[39m {k: [] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m load_stops()]}\n\u001b[0;32m      5\u001b[0m     c \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_stop_times' is not defined"
     ]
    }
   ],
   "source": [
    "all_occ_numbers()\n",
    "all_neighbours()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_all_neighbours():\n",
    "    with open(\"lvb_auswertung/neighbours.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finden indirekter Nachbarn\n",
    "- wofür werden diese Daten benötigt: räumlich nah beieinander liegende Haltestellen sollen für die Heatmap beachtet werden\n",
    "- andere Idee für die Heatmap:\n",
    "  - Nur Haltestellen mit bestimmtem Maximalabstand werden beachtet\n",
    "  - diese werden anhand von quadratischen Rastern mit der Seitenlänge als Abstand ermittelt (Stationen mit Abstand kleiner Maximalabstand müssen dann in benachbarten Kästchen liegen (beide Kästchenkoordinaten dürfen sich nur um maximal 1 unterscheiden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4377/4377 [00:00<00:00, 118195.43it/s]\n",
      "100%|██████████| 4377/4377 [00:00<00:00, 28408.96it/s]\n",
      "100%|██████████| 4377/4377 [00:00<00:00, 8485.93it/s]\n",
      "100%|██████████| 4377/4377 [00:01<00:00, 3175.13it/s]\n",
      "100%|██████████| 4377/4377 [00:03<00:00, 1402.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# That is waaay to sow and doesnt seem to do the job...\n",
    "# Issue found: when we access neighbour_data[cur_neighbour] we actually sometimes load the added neighbours...\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def indirect_neighbours(level):\n",
    "    base_data = {k: [elem[0] for elem in v] for k, v in load_all_neighbours().items()}\n",
    "    # wir nutzen eine Kopie der alten Daten, um nicht auf schon bearbeitete Nachbarn zuzugreifen, sondern auf die Ausgangsdaten aus einem Level\n",
    "    neighbour_data = deepcopy(base_data)\n",
    "    new_data = {}\n",
    "    for i in range(level):\n",
    "        new_data = deepcopy(neighbour_data)\n",
    "        # loop through all stations\n",
    "        for cur_station in tqdm(neighbour_data):\n",
    "            # loop through all current neighbours of that station\n",
    "            for cur_neighbour in base_data[cur_station]:\n",
    "                # loop through all neighbours of that neighbour\n",
    "                for neighbour_neighbour in neighbour_data[cur_neighbour]:\n",
    "                    if neighbour_neighbour not in new_data[cur_station]:\n",
    "                        new_data[cur_station].append(neighbour_neighbour)\n",
    "        neighbour_data = new_data\n",
    "    \n",
    "    with open(f\"lvb_auswertung/neighbours_{level}.json\", \"w+\") as f:\n",
    "        json.dump(neighbour_data, f, indent=4)\n",
    "    \n",
    "    return neighbour_data\n",
    "# bis level 3 ist es ok, level 4 habe ich nach 2.03it/s abgebrochen (progress: 35/4377, t=00:17)\n",
    "c = indirect_neighbours(5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umwandeln der Daten in einen Graphen:\n",
    "\n",
    "(kopiert von Lauris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000145', '0011276', '0011277', '0011274']\n",
      "180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x1a69a00b670>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as n\n",
    "import json\n",
    "\n",
    "def load_graph():\n",
    "    # parse into weighted graph\n",
    "    with open('lvb_auswertung/neighbours.json', \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    g = n.Graph()\n",
    "\n",
    "    for key_stop_id, neighbours in data.items():\n",
    "        for neighbour_stop_id, time in neighbours:\n",
    "            g.add_edge(key_stop_id, neighbour_stop_id, weight = time)\n",
    "\n",
    "    path = n.dijkstra_path(g,\"0000145\", \"0011274\")\n",
    "    time = n.path_weight(g, path, weight = \"weight\")\n",
    "\n",
    "    print(path)\n",
    "    print(time)\n",
    "    \n",
    "    return g\n",
    "\n",
    "load_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the stops by how many neigbours they have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      stop_id   stop_lat   stop_lon\n",
      "0         145  51.361559  12.446591\n",
      "1         152  51.340545  12.402030\n",
      "2         154  51.351856  12.286928\n",
      "3         163  51.376188  12.365049\n",
      "4         164  51.299198  12.392830\n",
      "...       ...        ...        ...\n",
      "4372  8080610  51.503651  11.970346\n",
      "4373  8080620  51.473314  11.921056\n",
      "4374  8080840  51.367074  12.366024\n",
      "4375  8080930  51.345178  12.424666\n",
      "4376  8098205  51.345696  12.380428\n",
      "\n",
      "[4377 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (stop_number \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     14\u001b[0m     all_stops\u001b[39m.\u001b[39msetdefault(i, [])\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, _ , lat, lon \u001b[39min\u001b[39;00m stops:\n\u001b[0;32m     17\u001b[0m     all_stops[\u001b[39mid\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(lat), \u001b[39mfloat\u001b[39m(lon)]\n\u001b[0;32m     19\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlvb_auswertung/neighbours_1.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import load_stops\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "# load stops\n",
    "stops = load_stops()\n",
    "print(stops)\n",
    "stop_number = len(stops)\n",
    "\n",
    "# prepare dictionary so we can just look up coordinates\n",
    "all_stops = {}\n",
    "for i in range (stop_number + 1):\n",
    "    all_stops.setdefault(i, [])\n",
    "\n",
    "for id, lat, lon in stops:\n",
    "    all_stops[id] = [float(lat), float(lon)]\n",
    "\n",
    "f = open(\"lvb_auswertung/neighbours_1.json\")\n",
    "nb = json.load(f)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "w = []\n",
    "for key_stop, nb_list in nb.items():\n",
    "    w.append(len(nb_list))\n",
    "    x1, y1 = all_stops[key_stop]\n",
    "    x.append(x1)\n",
    "    y.append(y1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "cm = plt.cm.get_cmap('inferno')\n",
    "\n",
    "#plt.xticks([50,52])\n",
    "#plt.yticks([12.2,12.6])\n",
    "plot = plt.scatter(x,y,c = w, s = 10, cmap = cm, alpha = 1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "das ganze als heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m BLUR \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      6\u001b[0m BINS \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m \u001b[39m# how detailed\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m      9\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhistogram2d(np\u001b[39m.\u001b[39marray(x), np\u001b[39m.\u001b[39marray(y),weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(w), bins\u001b[39m=\u001b[39mBINS)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m data \u001b[39m=\u001b[39m gaussian_filter(data, sigma\u001b[39m=\u001b[39mBLUR)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "BLUR = 10\n",
    "BINS = 500 # how detailed\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data = np.histogram2d(np.array(x), np.array(y),weights = np.array(w), bins=BINS)[0]\n",
    "data = gaussian_filter(data, sigma=BLUR)\n",
    "plt.pcolormesh(data.T, cmap='inferno', shading='gouraud')\n",
    "#ax.set_xticks([])\n",
    "#ax.set_yticks([])\n",
    "fig.canvas.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56db51791f5af84c2c46b29c05e486f714d52add8b6bfdb4d473ae47122d1c76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
